{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMFH9StefUnbrHh+FXkyJss",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mohammadhosseinkarimi/DQN_V1/blob/main/DQN_V1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whGtuiaGI5D-",
        "outputId": "037044b6-750e-4066-da9c-c2f5bb6fc6cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pettingzoo in /usr/local/lib/python3.11/dist-packages (1.25.0)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from pettingzoo) (2.0.2)\n",
            "Requirement already satisfied: gymnasium>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from pettingzoo) (1.1.1)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=1.0.0->pettingzoo) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=1.0.0->pettingzoo) (4.13.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=1.0.0->pettingzoo) (0.0.4)\n"
          ]
        }
      ],
      "source": [
        "# first i create my environment\n",
        "# i use petting zoo to create environment for task scheduler\n",
        "# so first install petting zoo\n",
        "!pip install pettingzoo"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pettingzoo.utils.env import AECEnv #to have Agent Environment cycle\n",
        "from gymnasium import spaces\n",
        "import numpy as np\n",
        "import functools\n",
        "\n",
        "import gymnasium\n",
        "from gymnasium.spaces import Discrete\n",
        "from gymnasium.utils import seeding\n",
        "\n",
        "from pettingzoo import ParallelEnv\n",
        "from pettingzoo.utils import parallel_to_aec, wrappers\n",
        "\n",
        "\n",
        "num_machines = 2    # number of agent\n",
        "num_resources = 3   # e.g CPU, RAM , GPU\n",
        "max_queue = 10      # max tasks in queue\n",
        "max_resource = 10   # max number of resource that agent have\n",
        "max_priority = 5    # priority for tasks between 1 to 5\n",
        "max_deadline = 20   # max deadline for every task\n",
        "total_CPU = 5       # number of CPUs\n",
        "total_RAM = 5       # number of RAMS\n",
        "total_GPU = 5       # number of GPU\n",
        "\n"
      ],
      "metadata": {
        "id": "bgRDskudSupD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TaskSchedulingEnv(AECEnv):\n",
        "    metadata = {\"render_modes\": [\"human\"], \"name\": \"task_scheduler_v1\"}\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.current_time = None\n",
        "        self.resources = None\n",
        "        self.running_tasks = None\n",
        "        self.future_tasks = None\n",
        "        self.task_queue = None\n",
        "        self.rewards = None\n",
        "        self.terminations = None\n",
        "        self.truncations = None\n",
        "        self.infos = None\n",
        "        self._agent_selector = None\n",
        "        self.agent_selection = None\n",
        "\n",
        "        self.agents = [f\"agent_{i}\" for i in range(num_machines)]\n",
        "        self.possible_agents = self.agents[:]\n",
        "\n",
        "        # this create the base of observasion for any agent\n",
        "        # we want a matrix that show\n",
        "        RESOURCE_POOL = ['cpu_1', 'cpu_2', 'cpu_3', 'ram_1', 'ram_2', 'ram_3', 'ram_4', 'ram_5']\n",
        "        NUM_TOTAL_RESOURCES = len(RESOURCE_POOL)\n",
        "\n",
        "\n",
        "        obs_space_resources = spaces.MultiBinary(NUM_TOTAL_RESOURCES)\n",
        "\n",
        "\n",
        "        # ب) فضای مربوط به تسک‌ها (ماتریس MAX_TASKS × NUM_TASK_FEATS)\n",
        "        #   هر ویژگی: req_CPU, req_RAM, duration, deadline_rem, priority → 5 ویژگی\n",
        "        NUM_TASK_FEATS = NUM_RESOURCES + 3  # req هر منبع + duration + deadline_rem + priority\n",
        "        obs_space_tasks = spaces.Box(\n",
        "            low=0,\n",
        "            high=MAX_TASK_FEATURE_VALUE,\n",
        "            shape=(MAX_TASKS, NUM_TASK_FEATS),\n",
        "            dtype=np.int32\n",
        "        )\n",
        "\n",
        "        # ج) فضای نهاییِ مشاهده به صورت Dict\n",
        "        obs_space_dict = spaces.Dict({\n",
        "            \"resources\": obs_space_resources,  # یک بردار 2تایی (CPU, RAM)\n",
        "            \"tasks\": obs_space_tasks           # ماتریس (MAX_TASKS × 5)\n",
        "        })\n",
        "\n",
        "        # حالا برای هر عامل همین فضای مشاهده را اختصاص می‌دهیم\n",
        "        self.observation_spaces = {\n",
        "            agent: obs_space_dict\n",
        "            for agent in self.agents\n",
        "        }\n",
        "\n",
        "        # ---------------- تعریف فضای عمل ----------------\n",
        "        # هر عامل می‌تواند {0, 1, ..., MAX_TASKS} را انتخاب کند:\n",
        "        #    0 = skip، 1..MAX_TASKS = تخصیص تسک از صف\n",
        "        self.action_spaces = {\n",
        "            agent: spaces.Discrete(MAX_TASKS + 1)\n",
        "            for agent in self.agents\n",
        "        }\n",
        "\n",
        "        # Action space: select task index or 0 for \"no action\"\n",
        "        self.action_spaces = {\n",
        "            agent: spaces.Discrete(MAX_TASKS + 1)\n",
        "            for agent in self.agents\n",
        "        }\n",
        "\n",
        "        self.reset()\n",
        "\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        self.agent_idx = 0\n",
        "        self.agents = self.possible_agents[:]\n",
        "        self.resources = np.full((NUM_MACHINES, NUM_RESOURCES), MAX_RESOURCE)\n",
        "        self.tasks = self._generate_tasks()\n",
        "        self.task_queue = self.tasks.copy()\n",
        "        self._agent_selector = iter(self.agents)\n",
        "        self.agent_selection = next(self._agent_selector)\n",
        "\n",
        "        self.rewards = {agent: 0 for agent in self.agents}\n",
        "        self.terminations = {agent: False for agent in self.agents}\n",
        "        self.truncations = {agent: False for agent in self.agents}\n",
        "        self.infos = {agent: {} for agent in self.agents}\n",
        "\n",
        "    def observe(self, agent):\n",
        "        i = int(agent.split('_')[1])\n",
        "        flat_resources = self.resources[i].tolist()\n",
        "        flat_tasks = []\n",
        "        for task in self.task_queue:\n",
        "            flat_tasks += task['resources'] + [task['duration']]\n",
        "        while len(flat_tasks) < MAX_TASKS * (NUM_RESOURCES + 1):\n",
        "            flat_tasks += [0] * (NUM_RESOURCES + 1)\n",
        "        return np.array(flat_resources + flat_tasks, dtype=np.int32)\n",
        "\n",
        "    def _generate_tasks(self):\n",
        "        return [\n",
        "            {\n",
        "                \"resources\": list(np.random.randint(1, MAX_RESOURCE // 2, size=NUM_RESOURCES)),\n",
        "                \"duration\": np.random.randint(1, 5)\n",
        "            }\n",
        "            for _ in range(MAX_TASKS)\n",
        "        ]\n",
        "\n",
        "    def step(self, action):\n",
        "        agent = self.agent_selection\n",
        "        i = int(agent.split('_')[1])\n",
        "\n",
        "        reward = 0\n",
        "        if action > 0 and action <= len(self.task_queue):\n",
        "            task = self.task_queue[action - 1]\n",
        "            if all(task[\"resources\"][j] <= self.resources[i][j] for j in range(NUM_RESOURCES)):\n",
        "                self.resources[i] -= task[\"resources\"]\n",
        "                self.task_queue.pop(action - 1)\n",
        "                reward = 1  # reward for successful scheduling\n",
        "\n",
        "        self.rewards[agent] = reward\n",
        "\n",
        "        try:\n",
        "            self.agent_selection = next(self._agent_selector)\n",
        "        except StopIteration:\n",
        "            self._agent_selector = iter(self.agents)\n",
        "            self.agent_selection = next(self._agent_selector)\n",
        "\n",
        "    def render(self):\n",
        "        for i, res in enumerate(self.resources):\n",
        "            print(f\"Machine {i} resources: {res}\")\n",
        "        print(f\"Queue: {self.task_queue}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "7qznzNjOLvQm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "outputId": "33833f1c-de2b-41b9-9474-170ee365e998"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'AECEnv' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-157b4da2ec1a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mTaskSchedulingEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAECEnv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"render_modes\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"human\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"name\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"task_scheduler_v1\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'AECEnv' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def dirichlet_integer_partition(total, num_parts, alpha=2.0):\n",
        "    \"\"\"\n",
        "    تخصیص 'total' واحد از یک منبع به 'num_parts' ماشین با استفاده از توزیع Dirichlet و سپس گرد کردن.\n",
        "    با alpha بزرگ‌تر (مثلاً 2.0) تقسیم به نحو متوازن‌تر انجام می‌شود.\n",
        "    خروجی: لیستی از اعداد صحیح به طول num_parts که جمعشان برابر total است.\n",
        "    \"\"\"\n",
        "    alpha_vector = np.ones(num_parts) * alpha\n",
        "    frac = np.random.dirichlet(alpha_vector)\n",
        "    real_vals = frac * total\n",
        "    floored = np.floor(real_vals).astype(int)\n",
        "    remainder = int(total - floored.sum())\n",
        "\n",
        "    if remainder > 0:\n",
        "        fractions = real_vals - floored\n",
        "        idx_desc = np.argsort(-fractions)\n",
        "        for i in range(remainder):\n",
        "            floored[idx_desc[i]] += 1\n",
        "\n",
        "    return floored.tolist()\n",
        "\n",
        "# ====================== تعریف کلاس محیط ======================\n",
        "\n",
        "class TaskSchedulingEnv(AECEnv):\n",
        "    metadata = {\n",
        "        \"render_modes\": [\"human\"],\n",
        "        \"name\": \"task_scheduler_v3\"\n",
        "    }\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # ۱. تعریف لیست Agentها (هر ماشین یک Agent)\n",
        "        self.agents = [f\"agent_{i}\" for i in range(NUM_MACHINES)]\n",
        "        self.possible_agents = self.agents[:]\n",
        "\n",
        "        # ۲. تعریف فضای مشاهده (Observation Space) برای هر Agent\n",
        "        #    هر Observation شامل:\n",
        "        #      - remaining_resources_of_machine_i (NUM_RESOURCES)\n",
        "        #      - current_time (1)\n",
        "        #      - وضعیت صف انتظار: برای MAX_TASKS تسک، هرکدام:\n",
        "        #          [req_CPU, req_RAM, duration, deadline_remaining, priority]  => (NUM_RESOURCES + 3)\n",
        "        #    => مجموع طول بردار: NUM_RESOURCES + 1 + MAX_TASKS * (NUM_RESOURCES + 3)\n",
        "        obs_dim = NUM_RESOURCES + 1 + MAX_TASKS * (NUM_RESOURCES + 3)\n",
        "        high_val = max(TOTAL_CPU, TOTAL_RAM, MAX_DEADLINE, MAX_PRIORITY)\n",
        "        self.observation_spaces = {\n",
        "            agent: spaces.Box(\n",
        "                low=0,\n",
        "                high=high_val,\n",
        "                shape=(obs_dim,),\n",
        "                dtype=np.int32\n",
        "            )\n",
        "            for agent in self.agents\n",
        "        }\n",
        "\n",
        "        # ۳. تعریف فضای عمل (Action Space) برای هر Agent\n",
        "        #    هر عدد از 0 تا MAX_TASKS معانی زیر را دارد:\n",
        "        #      0 = skip (عدم اقدام)\n",
        "        #      k (1 <= k <= len(task_queue)) = انتخاب تسک kام از صف\n",
        "        self.action_spaces = {\n",
        "            agent: spaces.Discrete(MAX_TASKS + 1)\n",
        "            for agent in self.agents\n",
        "        }\n",
        "\n",
        "        # ۴. متغیرهای حالت داخلی که در reset مقداردهی می‌شوند\n",
        "        self.current_time = None     # شمارندهٔ زمان شبیه‌سازی\n",
        "        self.resources = None        # np.ndarray شکل (NUM_MACHINES, NUM_RESOURCES)\n",
        "        self.running_tasks = None    # لیست دیکشنری‌های {machine, finish_time, task}\n",
        "        self.future_tasks = None     # لیست تمام taskهایی که هنوز ARRIVAL نشده‌اند\n",
        "        self.task_queue = None       # لیست taskهایی که وارد صف شده‌اند\n",
        "        self.rewards = None          # دیکشنری agent-> float\n",
        "        self.terminations = None     # دیکشنری agent-> bool\n",
        "        self.truncations = None      # دیکشنری agent-> bool (در این مثال استفاده نمی‌شود)\n",
        "        self.infos = None            # دیکشنری agent-> dict\n",
        "\n",
        "        self._agent_selector = None  # iterator برای ترتیب Agentها\n",
        "        self.agent_selection = None  # نام Agentی که در نوبت است\n",
        "\n",
        "    # ۴. متد کمکی برای تولید همهٔ تسک‌های اولیه با ویژگی‌های کامل\n",
        "    def _generate_all_tasks(self):\n",
        "        \"\"\"\n",
        "        تولید 2*MAX_TASKS تسک تصادفی:\n",
        "          - arrival_time: عددی در [0, MAX_DEADLINE//2]\n",
        "          - resources: [cpu_req, ram_req] هرکدام در [1, max(TOTAL_CPU, TOTAL_RAM)//2]\n",
        "          - duration: عددی در [1, 4]\n",
        "          - deadline: عددی >= arrival + duration تا حداکثر MAX_DEADLINE\n",
        "          - priority: عددی در [1, MAX_PRIORITY]\n",
        "        خروجی: لیستی از دیکشنری‌های task، مرتب‌شده بر اساس arrival_time\n",
        "        \"\"\"\n",
        "        all_tasks = []\n",
        "        num_total = 2 * MAX_TASKS\n",
        "        for _ in range(num_total):\n",
        "            arrival = np.random.randint(0, MAX_DEADLINE // 2)\n",
        "            resource_req = np.random.randint(1, max(TOTAL_CPU, TOTAL_RAM) // 2, size=NUM_RESOURCES).tolist()\n",
        "            duration = np.random.randint(1, 5)\n",
        "            min_dead = arrival + duration\n",
        "            max_dead = min_dead + (MAX_DEADLINE - min_dead)\n",
        "            deadline = np.random.randint(min_dead, max_dead + 1) if max_dead > min_dead else min_dead\n",
        "            priority = np.random.randint(1, MAX_PRIORITY + 1)\n",
        "\n",
        "            task = {\n",
        "                \"resources\": resource_req,\n",
        "                \"arrival_time\": arrival,\n",
        "                \"duration\": duration,\n",
        "                \"deadline\": deadline,\n",
        "                \"priority\": priority\n",
        "            }\n",
        "            all_tasks.append(task)\n",
        "\n",
        "        # مرتب‌سازی بر اساس arrival_time\n",
        "        all_tasks.sort(key=lambda t: t[\"arrival_time\"])\n",
        "        return all_tasks\n",
        "\n",
        "    # ۵. متد reset برای شروع اپیزود جدید\n",
        "    def reset(self, seed=None, options=None):\n",
        "        super().reset(seed=seed)\n",
        "\n",
        "        # ۵.۱. صفر کردن زمان شبیه‌سازی\n",
        "        self.current_time = 0\n",
        "\n",
        "        # ۵.۲. تخصیص خودکار منابع (CPU و RAM) به NUM_MACHINES ماشین\n",
        "        #      از روش Dirichlet-based با alpha=2.0 استفاده می‌کنیم تا تخصیص نسبتاً مساوی باشد.\n",
        "        cpu_partition = dirichlet_integer_partition(TOTAL_CPU, NUM_MACHINES, alpha=2.0)\n",
        "        ram_partition = dirichlet_integer_partition(TOTAL_RAM, NUM_MACHINES, alpha=2.0)\n",
        "\n",
        "        self.resources = np.zeros((NUM_MACHINES, NUM_RESOURCES), dtype=np.int32)\n",
        "        for i in range(NUM_MACHINES):\n",
        "            self.resources[i, 0] = cpu_partition[i]\n",
        "            self.resources[i, 1] = ram_partition[i]\n",
        "\n",
        "        # ۵.۳. تولید تسک‌های آینده و خالی کردن صف\n",
        "        self.future_tasks = self._generate_all_tasks()\n",
        "        self.task_queue = []\n",
        "\n",
        "        # ۵.۴. هیچ تسکی در حال اجرا نیست\n",
        "        self.running_tasks = []\n",
        "\n",
        "        # ۵.۵. مقداردهی اولیهٔ پاداش و پایانی‌ها و infos\n",
        "        self.rewards = {agent: 0.0 for agent in self.agents}\n",
        "        self.terminations = {agent: False for agent in self.agents}\n",
        "        self.truncations = {agent: False for agent in self.agents}  # در این مثال استفاده‌ای ندارد\n",
        "        self.infos = {agent: {} for agent in self.agents}\n",
        "\n",
        "        # ۵.۶. آماده‌سازی Iterator برای ترتیب Agentها\n",
        "        self._agent_selector = iter(self.agents)\n",
        "        self.agent_selection = next(self._agent_selector)\n",
        "\n",
        "    # ۶. متد کمکی برای اضافه‌کردن تسک‌های تازه به صف (بر اساس current_time)\n",
        "    def _add_arrived_tasks_to_queue(self):\n",
        "        \"\"\"\n",
        "        هر تسکی که arrival_time <= current_time باشد وارد صف می‌شود،\n",
        "        تا زمانی که ظرفیت صف (MAX_TASKS) پر نشده باشد.\n",
        "        \"\"\"\n",
        "        while self.future_tasks and self.future_tasks[0][\"arrival_time\"] <= self.current_time:\n",
        "            task = self.future_tasks.pop(0)\n",
        "            if len(self.task_queue) < MAX_TASKS:\n",
        "                self.task_queue.append(task)\n",
        "            else:\n",
        "                # اگر صف پر باشد، فعلاً آن تسک را نگه می‌داریم (و منتظر زمان بعدی می‌ماند)\n",
        "                # برای سادگی در این مدل، اگر صف پر باشد، از future_tasks حذف نمی‌کنیم.\n",
        "                break\n",
        "\n",
        "    # ۷. متد کمکی برای آزادسازی منابع تسک‌های تمام‌شده\n",
        "    def _release_completed_tasks(self):\n",
        "        \"\"\"\n",
        "        هر Entry در running_tasks که finish_time <= current_time باشد،\n",
        "        منابعش را به ماشین مربوطه باز می‌گرداند و از لیست running_tasks حذف می‌کند.\n",
        "        \"\"\"\n",
        "        still_running = []\n",
        "        for entry in self.running_tasks:\n",
        "            if entry[\"finish_time\"] <= self.current_time:\n",
        "                m_idx = entry[\"machine\"]\n",
        "                t = entry[\"task\"]\n",
        "                # آزادسازی منابع\n",
        "                self.resources[m_idx] += np.array(t[\"resources\"], dtype=np.int32)\n",
        "            else:\n",
        "                still_running.append(entry)\n",
        "        self.running_tasks = still_running\n",
        "\n",
        "    # ۸. متد observe: تولید بردار مشاهده برای یک Agent مشخص\n",
        "    def observe(self, agent):\n",
        "        \"\"\"\n",
        "        خروجی: برداری از طول ثابت = NUM_RESOURCES + 1 + MAX_TASKS * (NUM_RESOURCES + 3)\n",
        "        ساختار:\n",
        "          [ rem_cpu_of_machine_i, rem_ram_of_machine_i,\n",
        "            current_time,\n",
        "            task_1_req_cpu, task_1_req_ram, task_1_duration, task_1_deadline_rem, task_1_priority,\n",
        "            task_2_..., ...,\n",
        "            تا MAX_TASKS تسک؛ اگر صف کمتر باشد، با صفر پد می‌شود.\n",
        "          ]\n",
        "        \"\"\"\n",
        "        i = int(agent.split('_')[1])  # شماره ماشین مربوط به این Agent\n",
        "\n",
        "        # ۸.۱. منابع باقی‌ماندهٔ ماشین i\n",
        "        flat_resources = self.resources[i].tolist()\n",
        "\n",
        "        # ۸.۲. زمان فعلی\n",
        "        tnow = [self.current_time]\n",
        "\n",
        "        # ۸.۳. وضعیت صف انتظار\n",
        "        flat_tasks = []\n",
        "        for task in self.task_queue:\n",
        "            flat_tasks.extend(task[\"resources\"])                 # نیازهای منابع [cpu_req, ram_req]\n",
        "            flat_tasks.append(task[\"duration\"])                  # مدت اجرا\n",
        "            rem_dead = max(0, task[\"deadline\"] - self.current_time)\n",
        "            flat_tasks.append(rem_dead)                           # زمان باقیمانده تا ددلاین\n",
        "            flat_tasks.append(task[\"priority\"])                   # اولویت\n",
        "\n",
        "        # اگر تعداد تسک‌های صف < MAX_TASKS، با صفر پر می‌کنیم\n",
        "        per_task_dim = NUM_RESOURCES + 3\n",
        "        needed_padding = MAX_TASKS * per_task_dim - len(flat_tasks)\n",
        "        if needed_padding > 0:\n",
        "            flat_tasks.extend([0] * needed_padding)\n",
        "\n",
        "        obs = np.array(flat_resources + tnow + flat_tasks, dtype=np.int32)\n",
        "        return obs\n",
        "\n",
        "    # ۹. متد step: دریافت یک action از Agent و به‌روزرسانی محیط\n",
        "    def step(self, action):\n",
        "        \"\"\"\n",
        "        دریافت action از Agent self.agent_selection:\n",
        "          - action=0: skip (عدم اقدام)\n",
        "          - action=k (1 <= k <= len(task_queue)): انتخاب تسک kام از صف برای اختصاص منابع\n",
        "        اگر منابع کامل در ماشین i آزاد باشد:\n",
        "          - منابع را یکجا کم می‌کنیم (All-or-Nothing Allocation)\n",
        "          - ثبت finish_time = current_time + duration\n",
        "          - محاسبه پاداش بر مبنای deadline و priority\n",
        "          - حذف تسک از صف و اضافه کردن به running_tasks\n",
        "        اگر منابع کافی نباشد:\n",
        "          - پاداش منفی کوچک\n",
        "          - تسک در صف باقی می‌ماند\n",
        "        سپس:\n",
        "          - انتخاب Agent بعدی (نوبتی)\n",
        "          - اگر یک «دور کامل» عامل‌ها تمام شده باشد:\n",
        "               * current_time += 1\n",
        "               * آزادسازی منابع تسک‌هایی که تمام شدند\n",
        "               * اضافه کردن تسک‌های جدید به صف بر اساس arrival_time\n",
        "          - بررسی شرایط پایان اپیزود:\n",
        "            اگر task_queue، running_tasks و future_tasks همگی خالی باشند، اپیزود تمام است.\n",
        "        \"\"\"\n",
        "\n",
        "        agent = self.agent_selection\n",
        "        i = int(agent.split('_')[1])  # شماره ماشین که این Agent مأمور تصمیم‌گیری است\n",
        "\n",
        "        reward = 0.0\n",
        "\n",
        "        # ==== ۹.۱. برسی و اعمال action ====\n",
        "        if 1 <= action <= len(self.task_queue):\n",
        "            task = self.task_queue[action - 1]\n",
        "\n",
        "            # ۹.۱.۱. چک All-or-Nothing: آیا هر دو منابع (CPU و RAM) آزاد است؟\n",
        "            if all(task[\"resources\"][r] <= self.resources[i][r] for r in range(NUM_RESOURCES)):\n",
        "                # منابع کافی است → All-or-Nothing Allocation\n",
        "                self.resources[i] -= np.array(task[\"resources\"], dtype=np.int32)\n",
        "\n",
        "                # ثبت زمان پایان تسک\n",
        "                finish_t = self.current_time + task[\"duration\"]\n",
        "                self.running_tasks.append({\n",
        "                    \"machine\": i,\n",
        "                    \"finish_time\": finish_t,\n",
        "                    \"task\": task\n",
        "                })\n",
        "\n",
        "                # محاسبه پاداش:\n",
        "                # اگر تسک پیش از ددلاین تمام شود، پاداش = priority؛ در غیر این صورت = -priority\n",
        "                if finish_t <= task[\"deadline\"]:\n",
        "                    reward = float(task[\"priority\"])\n",
        "                else:\n",
        "                    reward = float(-task[\"priority\"])\n",
        "\n",
        "                # حذف تسک از صف\n",
        "                self.task_queue.pop(action - 1)\n",
        "            else:\n",
        "                # منابع کافی نیست → پاداش منفی کوچک و تسک را در صف نگه می‌دارد\n",
        "                reward = -0.1\n",
        "        else:\n",
        "            # action == 0 یا خارج از محدوده → skip، پاداش صفر\n",
        "            reward = 0.0\n",
        "\n",
        "        # ثبت پاداش برای این Agent\n",
        "        self.rewards[agent] = reward\n",
        "\n",
        "        # ==== ۹.۲. انتخاب Agent بعدی ====\n",
        "        try:\n",
        "            next_agent = next(self._agent_selector)\n",
        "            self.agent_selection = next_agent\n",
        "            end_of_cycle = False\n",
        "        except StopIteration:\n",
        "            # یک دور کامل عامل‌ها تمام شد → دور جدید\n",
        "            self._agent_selector = iter(self.agents)\n",
        "            self.agent_selection = next(self._agent_selector)\n",
        "            end_of_cycle = True\n",
        "\n",
        "        # ==== ۹.۳. اگر یک دور کامل تمام شده باشد (end_of_cycle=True) ====\n",
        "        if end_of_cycle:\n",
        "            # افزایش زمان شبیه‌سازی\n",
        "            self.current_time += 1\n",
        "\n",
        "            # آزادسازی منابع تسک‌های تمام‌شده\n",
        "            self._release_completed_tasks()\n",
        "\n",
        "            # اضافه کردن تسک‌هایی که زمان ورودشان رسیده\n",
        "            self._add_arrived_tasks_to_queue()\n",
        "\n",
        "        # ==== ۹.۴. بررسی پایان اپیزود ====\n",
        "        if not self.task_queue and not self.running_tasks and not self.future_tasks:\n",
        "            for a in self.agents:\n",
        "                self.terminations[a] = True\n",
        "\n",
        "    # ۱۰. متد render: نمایش متنی وضعیت محیط\n",
        "    def render(self):\n",
        "        \"\"\"\n",
        "        چاپ وضعیت:\n",
        "          - Current Time\n",
        "          - منابع باقی‌ماندهٔ هر ماشین\n",
        "          - صف انتظار: هر تسک با [req, duration, deadline_rem, priority]\n",
        "          - تسک‌های در حال اجرا: هر Entry با [machine, finish_time, priority]\n",
        "          - تعداد تسک‌های آینده (future_tasks)\n",
        "        \"\"\"\n",
        "        print(\"========== Render Environment ==========\")\n",
        "        print(f\"Current Time: {self.current_time}\\n\")\n",
        "        for i in range(NUM_MACHINES):\n",
        "            print(f\"Machine {i} resources: CPU={self.resources[i,0]}, RAM={self.resources[i,1]}\")\n",
        "\n",
        "        print(\"\\nQueue:\")\n",
        "        for idx, task in enumerate(self.task_queue):\n",
        "            rem_dead = max(0, task[\"deadline\"] - self.current_time)\n",
        "            print(f\"  Task {idx+1}: req={task['resources']}, dur={task['duration']}, \"\n",
        "                  f\"deadline_rem={rem_dead}, prio={task['priority']}\")\n",
        "\n",
        "        print(\"\\nRunning Tasks:\")\n",
        "        for entry in self.running_tasks:\n",
        "            print(f\"  On Machine {entry['machine']}: finish={entry['finish_time']}, \"\n",
        "                  f\"prio={entry['task']['priority']}\")\n",
        "\n",
        "        print(f\"\\nFuture tasks remaining: {len(self.future_tasks)}\")\n",
        "        print(\"========================================\\n\")"
      ],
      "metadata": {
        "id": "dUcAd_dHgB24"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}